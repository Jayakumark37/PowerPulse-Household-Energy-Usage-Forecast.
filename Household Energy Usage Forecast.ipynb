1.Data Understanding and Exploration

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# Load dataset
# Assuming your dataset has columns similar to the UCI household power consumption dataset
df = pd.read_csv('/content/household_power_consumption.txt', sep=';', 
                 parse_dates={'Datetime': ['Date', 'Time']}, 
                 dayfirst=True,
                 low_memory=False)

# Initial exploration
print(f"Dataset shape: {df.shape}")
print(f"Columns: {df.columns}")
print(f"Data types: {df.dtypes}")
print(f"Missing values: {df.isnull().sum()}")

# Convert string values to numeric
for col in df.columns:
    if col != 'Datetime':
        df[col] = pd.to_numeric(df[col], errors='coerce')

# Basic statistics
print(df.describe())

# Visualize distribution of main variable
plt.figure(figsize=(10, 6))
sns.histplot(df['Global_active_power'], kde=True)
plt.title('Distribution of Global Active Power')
plt.show()

# Time series plot
plt.figure(figsize=(15, 6))
plt.plot(df['Datetime'], df['Global_active_power'])
plt.title('Global Active Power Over Time')
plt.xlabel('Date')
plt.ylabel('Global Active Power (kilowatts)')
plt.tight_layout()
plt.show()

# Correlation analysis
plt.figure(figsize=(12, 10))
correlation = df.corr()
sns.heatmap(correlation, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix')
plt.tight_layout()
plt.show()

2.Data Preprocessing

import pandas as pd
import numpy as np

# Load dataset with correct delimiter
df = pd.read_csv('/content/household_power_consumption.txt', sep=';', low_memory=False)

# Replace '?' with NaN for proper handling of missing values
df.replace('?', np.nan, inplace=True)

# Convert numeric columns to float (this will safely coerce invalid values to NaN)
numeric_cols = ['Global_active_power', 'Global_reactive_power', 'Voltage', 'Global_intensity', 
                'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']

df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')

# Now replace remaining NaN with 0
df.fillna(0, inplace=True)

# Convert 'Date' and 'Time' into a single datetime column if not already done
if 'Datetime' not in df.columns:
    if 'Date' in df.columns and 'Time' in df.columns:
        df['Datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], format='%d/%m/%Y %H:%M:%S')
        df.drop(columns=['Date', 'Time'], inplace=True)
    else:
        # If 'Datetime' already exists and isn't in the columns, skip this part
        df['Datetime'] = pd.to_datetime(df['Datetime'], errors='coerce')

# Set the datetime as index for time-based interpolation
df = df.set_index('Datetime')

# Interpolate missing data using time-based method
df = df.interpolate(method='time')

# For any remaining missing values at the start or end
df = df.fillna(method='bfill').fillna(method='ffill')

# Handle outliers using IQR method
def handle_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df[column] = df[column].clip(lower_bound, upper_bound)
    return df

# Apply outlier handling to numeric columns
for col in ['Global_active_power', 'Global_reactive_power', 'Voltage', 'Global_intensity']:
    df = handle_outliers(df, col)

# Extract datetime components
df['hour'] = df.index.hour
df['day'] = df.index.day
df['month'] = df.index.month
df['year'] = df.index.year
df['weekday'] = df.index.weekday
df['weekend'] = df['weekday'].apply(lambda x: 1 if x >= 5 else 0)

# Create cyclical time features to preserve the circular nature of time
df['hour_sin'] = np.sin(2 * np.pi * df['hour']/24)
df['hour_cos'] = np.cos(2 * np.pi * df['hour']/24)
df['month_sin'] = np.sin(2 * np.pi * df['month']/12)
df['month_cos'] = np.cos(2 * np.pi * df['month']/12)

# Check for missing values after treatment
missing_values = df.isnull().sum()
print(f"Missing values after treatment: {missing_values}")

3.Feature Engineering

# Resample to hourly data for consistency
df_hourly = df.resample('H').mean()

# Create rolling window features
for window in [3, 6, 12, 24]:
    df_hourly[f'rolling_mean_{window}h'] = df_hourly['Global_active_power'].rolling(window=window).mean()
    df_hourly[f'rolling_std_{window}h'] = df_hourly['Global_active_power'].rolling(window=window).std()

# Create lag features
for lag in [1, 24, 168]:  # 1 hour, 1 day, 1 week
    df_hourly[f'lag_{lag}h'] = df_hourly['Global_active_power'].shift(lag)

# Create day part features (morning, afternoon, evening, night)
df_hourly['day_part'] = pd.cut(
    df_hourly.index.hour,
    bins=[0, 6, 12, 18, 24],
    labels=['Night', 'Morning', 'Afternoon', 'Evening'],
    include_lowest=True
)
# Convert to dummy variables
day_part_dummies = pd.get_dummies(df_hourly['day_part'], prefix='day_part')
df_hourly = pd.concat([df_hourly, day_part_dummies], axis=1)

# Calculate power factors and other electrical relationships
df_hourly['power_factor'] = df_hourly['Global_active_power'] / np.sqrt(
    df_hourly['Global_active_power']**2 + df_hourly['Global_reactive_power']**2
)

# Calculate sub-meter total and residual
if all(col in df_hourly.columns for col in ['Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']):
    df_hourly['submeter_total'] = df_hourly['Sub_metering_1'] + df_hourly['Sub_metering_2'] + df_hourly['Sub_metering_3']
    # Convert submeter readings (in watt-hour) to kilowatt-hour for comparison with Global_active_power
    df_hourly['residual_power'] = df_hourly['Global_active_power'] - df_hourly['submeter_total']/1000

# Drop rows with NaN values resulting from the lag and rolling features
df_hourly = df_hourly.dropna()

4.Model Selection and Training

from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import xgboost as xgb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM
from tensorflow.keras.optimizers import Adam

# Prepare features and target
X = df_hourly.drop(['Global_active_power'], axis=1)
y = df_hourly['Global_active_power']

# For categorical columns, ensure they're removed or encoded
X = X.select_dtypes(exclude=['object', 'category'])

# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)

# Time-based train-test split (more appropriate for time series)
train_end = df_hourly.index.max() - pd.Timedelta(days=30)
X_train = X_scaled[X_scaled.index <= train_end]
X_test = X_scaled[X_scaled.index > train_end]
y_train = y[y.index <= train_end]
y_test = y[y.index > train_end]

print(f"Training set shape: {X_train.shape}")
print(f"Testing set shape: {X_test.shape}")

# Function to evaluate models
def evaluate_model(y_true, y_pred, model_name):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    
    print(f"{model_name} Performance:")
    print(f"RMSE: {rmse:.4f}")
    print(f"MAE: {mae:.4f}")
    print(f"R²: {r2:.4f}")
    return rmse, mae, r2

# Dictionary to store model results
model_results = {}

# 1. Linear Regression
lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)
model_results['Linear Regression'] = evaluate_model(y_test, y_pred_lr, "Linear Regression")

# 2. Ridge Regression
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)
y_pred_ridge = ridge.predict(X_test)
model_results['Ridge'] = evaluate_model(y_test, y_pred_ridge, "Ridge Regression")

# 3. Random Forest
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
model_results['Random Forest'] = evaluate_model(y_test, y_pred_rf, "Random Forest")

# 4. Gradient Boosting
gb = GradientBoostingRegressor(n_estimators=100, random_state=42)
gb.fit(X_train, y_train)
y_pred_gb = gb.predict(X_test)
model_results['Gradient Boosting'] = evaluate_model(y_test, y_pred_gb, "Gradient Boosting")

# 5. XGBoost
xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)
model_results['XGBoost'] = evaluate_model(y_test, y_pred_xgb, "XGBoost")

5.Model Evaluation and Hyperparameter Tuning

# Visualize model performance
models = list(model_results.keys())
rmse_values = [result[0] for result in model_results.values()]
mae_values = [result[1] for result in model_results.values()]
r2_values = [result[2] for result in model_results.values()]

plt.figure(figsize=(15, 5))

plt.subplot(131)
plt.bar(models, rmse_values)
plt.title('RMSE by Model')
plt.xticks(rotation=45)

plt.subplot(132)
plt.bar(models, mae_values)
plt.title('MAE by Model')
plt.xticks(rotation=45)

plt.subplot(133)
plt.bar(models, r2_values)
plt.title('R² by Model')
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

# Find the best performing model based on RMSE
best_model = models[np.argmin(rmse_values)]
print(f"Best performing model based on RMSE: {best_model}")

# Hyperparameter tuning for the best model
if best_model == 'Random Forest':
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
    model = RandomForestRegressor(random_state=42)
elif best_model == 'Gradient Boosting':
    param_grid = {
        'n_estimators': [50, 100, 200],
        'learning_rate': [0.01, 0.1, 0.2],
        'max_depth': [3, 5, 7],
        'min_samples_split': [2, 5]
    }
    model = GradientBoostingRegressor(random_state=42)
elif best_model == 'XGBoost':
    param_grid = {
        'n_estimators': [50, 100, 200],
        'learning_rate': [0.01, 0.1, 0.2],
        'max_depth': [3, 5, 7],
        'colsample_bytree': [0.7, 0.8, 0.9]
    }
    model = xgb.XGBRegressor(random_state=42)
else:
    # Linear models
    param_grid = {
        'alpha': [0.1, 1.0, 10.0, 100.0]
    }
    model = Ridge()

# Time series cross-validation
tscv = TimeSeriesSplit(n_splits=5)

# Grid search with time series CV
grid_search = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    cv=tscv,
    scoring='neg_root_mean_squared_error',
    verbose=1,
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

print(f"Best parameters: {grid_search.best_params_}")
best_model = grid_search.best_estimator_

# Evaluate the optimized model
y_pred_best = best_model.predict(X_test)
rmse, mae, r2 = evaluate_model(y_test, y_pred_best, f"Optimized {best_model}")

# Visualize predictions vs actual
plt.figure(figsize=(15, 6))
plt.plot(y_test.index, y_test.values, label='Actual')
plt.plot(y_test.index, y_pred_best, label='Predicted')
plt.title('Predicted vs Actual Power Consumption')
plt.xlabel('Date')
plt.ylabel('Global Active Power (kilowatts)')
plt.legend()
plt.tight_layout()
plt.show()

# Feature importance for tree-based models
if hasattr(best_model, 'feature_importances_'):
    feature_importance = pd.DataFrame({
        'Feature': X_train.columns,
        'Importance': best_model.feature_importances_
    }).sort_values('Importance', ascending=False)
    
    plt.figure(figsize=(12, 8))
    sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))
    plt.title('Top 15 Feature Importances')
    plt.tight_layout()
    plt.show()

6.Model Deployment and Forecasting

def forecast_next_24_hours(model, last_data, scaler):
    """
    Forecast the next 24 hours of power consumption
    
    Parameters:
    model: Trained model
    last_data: DataFrame with the most recent data
    scaler: Fitted scaler for feature normalization
    
    Returns:
    DataFrame with forecasted values
    """
    # Create a dataframe for the next 24 hours
    future_times = pd.date_range(
        start=last_data.index[-1] + pd.Timedelta(hours=1),
        periods=24,
        freq='H'
    )
    
    forecast_df = pd.DataFrame(index=future_times)
    
    # Add necessary time features
    forecast_df['hour'] = forecast_df.index.hour
    forecast_df['day'] = forecast_df.index.day
    forecast_df['month'] = forecast_df.index.month
    forecast_df['year'] = forecast_df.index.year
    forecast_df['weekday'] = forecast_df.index.weekday
    forecast_df['weekend'] = forecast_df['weekday'].apply(lambda x: 1 if x >= 5 else 0)
    
    # Add cyclical features
    forecast_df['hour_sin'] = np.sin(2 * np.pi * forecast_df['hour']/24)
    forecast_df['hour_cos'] = np.cos(2 * np.pi * forecast_df['hour']/24)
    forecast_df['month_sin'] = np.sin(2 * np.pi * forecast_df['month']/12)
    forecast_df['month_cos'] = np.cos(2 * np.pi * forecast_df['month']/12)
    
    # Start with initial lag values from most recent data
    forecast_df['lag_1h'] = list(last_data['Global_active_power'].values[-1:]) + [None] * 23
    forecast_df['lag_24h'] = list(last_data['Global_active_power'].values[-24:])
    
    # Iteratively predict each hour
    for i in range(1, 24):
        # Prepare features for the current prediction
        current_features = forecast_df.iloc[[i-1]].copy()
        
        # Remove the target variable if it exists
        if 'Global_active_power' in current_features.columns:
            current_features = current_features.drop('Global_active_power', axis=1)
        
        # Fill in any missing features based on your model's requirements
        # This depends on which features your model uses
        
        # Scale the features
        current_features_scaled = scaler.transform(current_features)
        
        # Make prediction
        prediction = model.predict(current_features_scaled)[0]
        
        # Update the lag features for the next prediction
        if i < 23:
            forecast_df.iloc[i+1, forecast_df.columns.get_loc('lag_1h')] = prediction
    
    # Make final predictions
    X_forecast = forecast_df.copy()
    # Handle any remaining NaN values
    X_forecast = X_forecast.fillna(method='ffill')
    # Scale
    X_forecast_scaled = scaler.transform(X_forecast)
    # Predict
    forecast_df['Global_active_power'] = model.predict(X_forecast_scaled)
    
    return forecast_df[['Global_active_power']]

# #Example usage (would execute after model training)
# forecast = forecast_next_24_hours(best_model, df_hourly.iloc[-24:], scaler)

#Visualize the forecast
plt.figure(figsize=(15, 6))
plt.plot(df_hourly.index[-48:], df_hourly['Global_active_power'].iloc[-48:], label='Historical')
# plt.plot(forecast.index, forecast['Global_active_power'], label='Forecast')
plt.title('24-Hour Power Consumption Forecast')
plt.xlabel('Date')
plt.ylabel('Global Active Power (kilowatts)')
plt.legend()
plt.tight_layout()
plt.show()
